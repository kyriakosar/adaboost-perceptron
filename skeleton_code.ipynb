{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install numpy\n",
    "# import os\n",
    "# print(\"Current working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries/packages here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Notes:\n",
    "1. Dataset 1: a linearly separable dataset where you can test the correctness of your base learner and boosting algorithms\n",
    "   \n",
    "   300 samples 2 features\n",
    "   \n",
    "   ![dataset1.png](./dataset1.png)\n",
    "   \n",
    "   Generally speaking, your learners shall 100% correctly classify the data in dataset 1.\n",
    "\n",
    "2. Dataset 2 ~ 4 : non-linearly separable cases, applying descent boosting techniques can be beneficial\n",
    "   \n",
    "   Dataset 2: 300 samples 2 features. In comparison to the performance of your single base learner, does your boosting algorithm perferm better?\n",
    "      \n",
    "   ![dataset2.png](./dataset2.png)\n",
    "      \n",
    "   Dataset 3: 400 samples 2 features (challenging)\n",
    "\n",
    "      A good classifier shall obtain a ellipse-like decision boundary on this dataset. Can your algorithms handle this dataset? If not, can you try to give reasonable explanations?\n",
    "\n",
    "   ![dataset3.png](./dataset3.png)\n",
    "\n",
    "   Dataset 4: 3000 samples 10 features (more challenging)\n",
    "   \n",
    "      This is more or less the higher dimensional version of dataset3. We visualize the first two features of dataset 3, As it is shown in the following figure, they are non-linearly separable. \n",
    "      \n",
    "      How do your algorithms perform?\n",
    "\n",
    "   ![dataset4.png](./dataset4.png)\n",
    "\n",
    "   \n",
    "3. The data is also provided in csv format:\n",
    "   1. Feature columns and a label column \n",
    "   \n",
    "HINTs: \n",
    "1. Split the data into two parts (i.e., training data and test data).\n",
    "2. Draw decision boundary (surface) of your classifiers (on dataset 1 & 2) can be helpful.\n",
    "3. Carefully design your experiments so that you can understand the influence of increasing or decreasing some parameters (e.g., learning rate, number of base learners in boosting Alg.)\n",
    "4. Make smart implementations (e.g., vectorization using numpy to avoid some nested-loops in python), so that you can efficiently run more experiments\n",
    "5. The performance of your classifiers is not of high priority in this assignment.\n",
    "   1. The datasets are all artificially generated (toy) data, in principle, there is no need to preprocess the data.\n",
    "   2. Constructive discussions on your findings are more important. If the results are not good, try to find out the reasons.\n",
    "   3. We hope this assignment can help you fill in the gap between theory and application.\n",
    "6. You are encouraged to implement not only Adaboost but also other boosting algorithms of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 2)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load the dataset\n",
    "Dataset (Numpy npz file)\n",
    "|- features (Numpy.ndarray)\n",
    "|- labels (Numpy.ndarray)\n",
    "\n",
    "The data is also provided in csv format.\n",
    "\"\"\"\n",
    "\n",
    "def load_data(file_name='dataset3.npz'):\n",
    "    \"\"\" Load the Numpy npz format dataset \n",
    "    Args:\n",
    "        file_name (string): name and path to the dataset (dataset1.npz, dataset2.npz, dataset3.npz)\n",
    "    Returns:\n",
    "        X (Numpy.ndarray): features\n",
    "        y (Numpy.ndarray): 1D labels\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    data = np.load(file_name)\n",
    "    X, y = data['features'], data['labels']\n",
    "    return X, y\n",
    "\n",
    "# Load dataset 1 by default\n",
    "X, y =load_data()\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skeleton codes:\n",
    "You should follow the structure of this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgen = np.random.RandomState(1)\n",
    "weights =  np.random.normal(loc=0.0, scale=0.01, size=(1 + X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    # Implement your base learner here\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, **kwargs):\n",
    "        \"\"\" Initialize the parameters here \n",
    "        Args:\n",
    "            learning_rate (float or a collection of floats): your learning rate\n",
    "            max_iter (int): the maximum number of training iterations\n",
    "            Other parameters of your choice\n",
    "        \n",
    "        Examples ToDos:\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        Try different initialization strategies (as required in Question 2.3)\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        # self.random_state = random_state\n",
    "    \n",
    "        pass\n",
    "\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\" Implement the training strategy here\n",
    "        Args:\n",
    "            X (Numpy.ndarray, list, etc.): The training data\n",
    "            y (Numpy.ndarray, list, etc.): The labels\n",
    "            Other parameters of your choice\n",
    "\n",
    "        Example ToDos:\n",
    "        # for _ in range(self.max_iter):\n",
    "        #     Update the parameters of Perceptron according to the learning rate (self.learning_rate) and data (X, y)\n",
    "        \"\"\"  \n",
    "       \n",
    "        # self.coef_ = np.random.uniform(0, 1, 1 + X.shape[1])\n",
    "        self.coef_ = np.random.normal(loc=0.0, scale=0.01, size=(1 + X.shape[1]))\n",
    "        self.errors_ = []\n",
    "        for _ in range(self.max_iter):\n",
    "            errors = 0\n",
    "            self.learning_rate = self.learning_rate * 0.5\n",
    "            \n",
    "            for xi, expected_value in zip(X, y):\n",
    "                predicted_value = self.predict(xi)\n",
    "                self.coef_[1:] = self.coef_[1:] + self.learning_rate * (expected_value - predicted_value) * xi\n",
    "                self.coef_[0] = self.coef_[0] + self.learning_rate * (expected_value - predicted_value) * 1\n",
    "                update = self.learning_rate * (expected_value - predicted_value)\n",
    "                errors += int(update != 0.0)\n",
    "\n",
    "            self.errors_.append(errors)\n",
    "        pass\n",
    "\n",
    "    def net_input(self, X):\n",
    "        weighted_sum = np.dot(X,self.coef_[1:]) + self.coef_[0]\n",
    "        return weighted_sum\n",
    "\n",
    "    def activation_function(self, X):\n",
    "        weighted_sum = self.net_input(X)\n",
    "        return np.where(weighted_sum >= 0.0, 1, -1)\n",
    "\n",
    "    def predict(self, x, **kwargs):\n",
    "        \"\"\" Implement the prediction strategy here\n",
    "        Args:\n",
    "            x (Numpy.ndarray, list, Numpy.array, etc.): The input data\n",
    "            Other parameters of your choice\n",
    "        Return(s):\n",
    "            The prediction value(s), namely, class label(s), others of your choice\n",
    "        \"\"\" \n",
    "        return self.activation_function(x)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        misclassified_data_count = 0\n",
    "        for xi, target in zip(X, y):\n",
    "            output = self.predict(xi)\n",
    "            if(target != output):\n",
    "                misclassified_data_count += 1\n",
    "        total_data_count = len(X)\n",
    "        self.score_ = (total_data_count - misclassified_data_count)/total_data_count\n",
    "        return self.score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Adaboost:\n",
    "#     def __init__(self, n_clf=5):\n",
    "#         self.n_clf = n_clf\n",
    "#         self.clfs = []\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         n_samples, n_features = X.shape\n",
    "\n",
    "#         # Initialize weights to 1/N\n",
    "#         w = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "#         self.clfs = []\n",
    "\n",
    "#         # Iterate through classifiers\n",
    "#         for _ in range(self.n_clf):\n",
    "#             clf = Perceptron()\n",
    "#             min_error = float(\"inf\")\n",
    "\n",
    "#             # greedy search to find best threshold and feature\n",
    "#             for feature_i in range(n_features):\n",
    "#                 X_column = X[:, feature_i]\n",
    "#                 thresholds = np.unique(X_column)\n",
    "\n",
    "#                 for threshold in thresholds:\n",
    "#                     # predict with polarity 1\n",
    "#                     p = 1\n",
    "#                     predictions = np.ones(n_samples)\n",
    "#                     predictions[X_column < threshold] = -1\n",
    "\n",
    "#                     # Error = sum of weights of misclassified samples\n",
    "#                     misclassified = w[y != predictions]\n",
    "#                     error = sum(misclassified)\n",
    "\n",
    "#                     if error > 0.5:\n",
    "#                         error = 1 - error\n",
    "#                         p = -1\n",
    "\n",
    "#                     # store the best configuration\n",
    "#                     if error < min_error:\n",
    "#                         clf.polarity = p\n",
    "#                         clf.threshold = threshold\n",
    "#                         clf.feature_idx = feature_i\n",
    "#                         min_error = error\n",
    "\n",
    "#             # calculate alpha\n",
    "#             EPS = 1e-10\n",
    "#             clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "\n",
    "#             # calculate predictions and update weights\n",
    "#             predictions = clf.predict(X.T, w)\n",
    "\n",
    "#             w *= np.exp(-clf.alpha * y * predictions.T)\n",
    "#             # Normalize to one\n",
    "#             w /= np.sum(w)\n",
    "\n",
    "#             # Save classifier\n",
    "#             self.clfs.append(clf)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
    "#         y_pred = np.sum(clf_preds, axis=0)\n",
    "#         y_pred = np.sign(y_pred)\n",
    "\n",
    "#         return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Perceptron:\n",
    "#     def __init__(self, learning_rate=0.05, n_iters=1000):\n",
    "#         self.lr = learning_rate\n",
    "#         self.n_iters = n_iters\n",
    "#         self.activation_func = self._unit_step_func\n",
    "#         self.weights = None\n",
    "#         self.bias = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         n_samples, n_features = X.shape\n",
    "\n",
    "#         # init parameters\n",
    "#         self.weights = np.zeros(n_features)\n",
    "#         self.bias = 0\n",
    "\n",
    "#         # y_ = np.array([1 if i > 0 else 0 for i in y])\n",
    "\n",
    "#         for _ in range(self.n_iters):\n",
    "\n",
    "#             for idx, x_i in enumerate(X):\n",
    "\n",
    "#                 linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "#                 y_predicted = self.activation_func(linear_output)\n",
    "\n",
    "#                 # Perceptron update rule\n",
    "#                 update = self.lr * (y[idx] - y_predicted)\n",
    "\n",
    "#                 self.weights += update * x_i\n",
    "#                 self.bias += update\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         linear_output = np.dot(X, self.weights) + self.bias\n",
    "#         y_predicted = self.activation_func(linear_output)\n",
    "#         return y_predicted\n",
    "\n",
    "#     def _unit_step_func(self, x):\n",
    "#         return np.where(x >= 0, 1, -1)\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         misclassified_data_count = 0\n",
    "#         for xi, target in zip(X, y):\n",
    "#             output = self.predict(xi)\n",
    "#             if(target != output):\n",
    "#                 misclassified_data_count += 1\n",
    "#         total_data_count = len(X)\n",
    "#         self.score_ = (total_data_count - misclassified_data_count)/total_data_count\n",
    "#         return self.score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(**kwargs):\n",
    "    \"\"\" Single run of your classifier\n",
    "    # Load the data\n",
    "    X, y = load_data()\n",
    "    # Find a way to split the data into training and test sets\n",
    "    -> X_train, y_train, X_test, y_test\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    base = Perceptron(\"your parameters\")\n",
    "    \n",
    "    # Train the classifier\n",
    "    base.fit(X_train, y_train, \"other parameters\")\n",
    "   \n",
    "    # Test and score the base learner using the test data\n",
    "\n",
    "    y_pred = base.predict(X_test, \"other parameters\")\n",
    "    score = SCORING(y_pred, y_test)\n",
    "    \"\"\"\n",
    "    X, y =load_data()\n",
    "    \n",
    "    # Create training and test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "    # weighted_sum = np.dot(X_train[0], self.coef_[1:]) + self.coef_[0]\n",
    "\n",
    "    prcptrn = Perceptron()\n",
    "    prcptrn.fit(X_train, y_train)\n",
    "   \n",
    "\n",
    "    print('Accuracy test: ',prcptrn.score(X_test, y_test),'Accuracy train: ', prcptrn.score(X_train, y_train))\n",
    "\n",
    "\n",
    "    # %matplotlib inline\n",
    "    # import matplotlib.pyplot as plt\n",
    " \n",
    "    # plt.plot(range(1, len(prcptrn.errors_) + 1), prcptrn.errors_, marker='o')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Number of updates')\n",
    "    \n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test:  0.38333333333333336 Accuracy train:  0.4142857142857143\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error rate, alpha and w\n",
    "def compute_error(y, preds, d_i):\n",
    "    '''\n",
    "    Calculate the error rate of a weak classifier m. Arguments:\n",
    "    y: actual target value\n",
    "    y_pred: predicted value by weak classifier\n",
    "    w_i: individual weights for each observation\n",
    "    \n",
    "    Note that all arrays should be the same length\n",
    "    '''\n",
    "    # min_error = float('inf')\n",
    "    \n",
    "    # misclassified = d_i[y!=preds]\n",
    "    # error=sum(misclassified)\n",
    "                    \n",
    "    # if error >0.5:\n",
    "    #     error=1-error\n",
    "                    \n",
    "    # if error<min_error:\n",
    "    #     min_error=error\n",
    "    # # \n",
    "    # min_error = np.argmin(sum(d_i * (np.not_equal(y, preds))))\n",
    "    \n",
    "    return sum(d_i * (np.not_equal(y, preds)).astype(int))/sum(d_i)\n",
    "\n",
    "def compute_alpha(min_error):\n",
    "    '''\n",
    "    Calculate the weight of a weak classifier m in the majority vote of the final classifier. This is called\n",
    "    alpha in chapter 10.1 of The Elements of Statistical Learning. Arguments:\n",
    "    error: error rate from weak classifier m\n",
    "    '''\n",
    "    EPS=1e-10\n",
    "    \n",
    "    # np.log((1 - min_error) / min_error)\n",
    "    return 0.5*np.log((1 - min_error+EPS) / (min_error + EPS))\n",
    "\n",
    "def update_distribution(d_i, alpha, y, preds, min_error):\n",
    "    ''' \n",
    "    Update individual weights w_i after a boosting iteration. Arguments:\n",
    "    w_i: individual weights for each observation\n",
    "    y: actual target value\n",
    "    y_pred: predicted value by weak classifier  \n",
    "    alpha: weight of weak classifier used to estimate y_pred\n",
    "    '''  \n",
    "    # d_i * np.exp(alpha * (np.not_equal(y, preds)).astype(int))\n",
    "    EPS=1e-10\n",
    "    return (d_i * np.exp(-alpha*y*preds))/(2*np.sqrt((min_error+EPS)*(1-min_error+EPS)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AdaBoost class\n",
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self, n_estimators, **kwargs):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.G_M = []\n",
    "        self.M = None\n",
    "        self.training_errors = []\n",
    "        self.prediction_errors = []\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "\n",
    "        '''\n",
    "        Fit model. Arguments:\n",
    "        X: independent variables - array-like matrix\n",
    "        y: target variable - array-like vector\n",
    "        M: number of boosting rounds. Default is 100 - integer\n",
    "        '''\n",
    "        M = self.n_estimators\n",
    "        # Clear before calling\n",
    "        self.alphas = [] \n",
    "        self.training_errors = []\n",
    "        self.M = M\n",
    "\n",
    "        # Iterate over M weak classifiers\n",
    "        for m in range(0, M):\n",
    "            \n",
    "            # Set weights for current boosting iteration\n",
    "            if m == 0:\n",
    "                d_i = np.ones(len(y)) * 1 / len(y)  # At m = 0, weights are all the same and equal to 1 / N\n",
    "            else:\n",
    "                # (d) Update w_i\n",
    "                d_i = update_distribution(d_i, alpha_m, y, preds, error_m)\n",
    "            \n",
    "            # (a) Fit weak classifier and predict labels\n",
    "            G_m = Perceptron()     # Stump: Two terminal-node classification tree\n",
    "            G_m.fit(X, y)\n",
    "            preds = G_m.predict(X)\n",
    "            # print(preds)\n",
    "            \n",
    "            self.G_M.append(G_m) # Save to list of weak classifiers\n",
    "\n",
    "            # (b) Compute error\n",
    "            error_m = compute_error(y, preds, d_i)\n",
    "            self.training_errors.append(error_m)\n",
    "            # print('Error:', error_m)\n",
    "            \n",
    "\n",
    "            # (c) Compute alpha\n",
    "            alpha_m = compute_alpha(error_m)\n",
    "            self.alphas.append(alpha_m)\n",
    "            # print('Alpha', alpha_m)\n",
    "            # print('------------------')\n",
    "\n",
    "        assert len(self.G_M) == len(self.alphas)\n",
    "        return self.alphas\n",
    "\n",
    "    def predict(self,X, alpha_m):\n",
    "        clf_preds = [alpha_m[i] * G_m.predict(X) for i, G_m in enumerate(self.G_M)]\n",
    "        y_pred = np.sum(clf_preds,axis=0)\n",
    "        y_pred = np.sign(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "        \n",
    "\n",
    "        # Calculate final predictions\n",
    "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(**kwargs):\n",
    "    \"\"\" Single run of your classifier\n",
    "    # Load the data\n",
    "    X, y = load_data()\n",
    "    # Find a way to split the data into training and test sets\n",
    "    -> X_train, y_train, X_test, y_test\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    base = Perceptron(\"your parameters\")\n",
    "    \n",
    "    # Train the classifier\n",
    "    base.fit(X_train, y_train, \"other parameters\")\n",
    "   \n",
    "    # Test and score the base learner using the test data\n",
    "\n",
    "    y_pred = base.predict(X_test, \"other parameters\")\n",
    "    score = SCORING(y_pred, y_test)\n",
    "    \"\"\"\n",
    "    X, y = load_data()\n",
    "    \n",
    "    # Create training and test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "    # weighted_sum = np.dot(X_train[0], self.coef_[1:]) + self.coef_[0]\n",
    "\n",
    "    ada_boost = AdaBoost()\n",
    "\n",
    "    ada_boost.fit(X_train, y_train)\n",
    "    y_pred_test = ada_boost.predict(X_test)\n",
    "    y_pred_train = ada_boost.predict(X_train)\n",
    "   \n",
    "    print('---------------------------------------------------------------------------------------')\n",
    "    print('Accuracy test: ',accuracy(y_test, y_pred_test),'Accuracy train: ', accuracy(y_train, y_pred_train))\n",
    "\n",
    "    # %matplotlib inline\n",
    "    # import matplotlib.pyplot as plt\n",
    " \n",
    "    # plt.plot(range(1, len(accuracy(X_test, y_test)) + 1), prcptrn.errors_, marker='o')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Number of updates')\n",
    "    \n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (280,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [301], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [300], line 31\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# weighted_sum = np.dot(X_train[0], self.coef_[1:]) + self.coef_[0]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m ada_boost \u001b[38;5;241m=\u001b[39m Adaboost()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mada_boost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m ada_boost\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     33\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m ada_boost\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "Cell \u001b[0;32mIn [299], line 52\u001b[0m, in \u001b[0;36mAdaboost.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# calculate predictions and update weights\u001b[39;00m\n\u001b[1;32m     50\u001b[0m predictions \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X\u001b[38;5;241m.\u001b[39mT, w)\n\u001b[0;32m---> 52\u001b[0m w \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Normalize to one\u001b[39;00m\n\u001b[1;32m     54\u001b[0m w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(w)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (280,) (2,) "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck with the assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('snacs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f3b8f08283fd47419890b63bd6aba7d477bc1d1dd10a4488ca2943378e61658"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
